return(y)
}
Z2016 <- Zmean(x = split2016)
Z2017 <- Zmean(x = split2017)
sites <- c(1:16)
specs <- logical()
for(i in 1:ncol(Z2016)){
specs[i] <- as.character(paste("Spec", i, sep = ""))
}
long.dat <- function(x){
colnames(x) <- specs
x$Sites <- sites
x %>%
gather(Spec1:Spec11, key = "Species", value = "OccProb")
}
long2016 <- long.dat(x = Z2016)
long2017 <- long.dat(x = Z2017)
library(devtools)
devtools::install_github("clauswilke/colorblindr")
update.packages(colorspace)
library(colorspace)
s
library(colorspaces)
update.packages(colorspace)
install.packages("colorspace")
install.packages("colorspace")
library(colorspace)
library(colorblindr)
devtools::install_github("clauswilke/colorblindr")
update.packages(colorspace)
install.packages("colorspace", repos="http://R-Forge.R-project.org")
install.packages("colorspace", repos = "http://R-Forge.R-project.org")
devtools::install_github("clauswilke/colorblindr")
install.packages("colorspace", repos = "http://R-Forge.R-project.org")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(metacom)
library(betapart)
library(tidyverse)
occplot1 <- ggplot(data = long2016, aes(x = Species, y = Sites, fill = OccProb))+
geom_tile()+
scale_fill_continuous(trans = 'reverse', color = "black")
occplot1 <- ggplot(data = long2016, aes(x = Species, y = Sites, fill = OccProb))+
geom_tile()+
scale_fill_gradient(low = "white", high = "black")
occplot1
Z2016 <- Zmean[-V9:V11]
Z2016 <- Zmean[,-c(9:11)]
Z2016 <- Zmean[,-c(V9, V11)]
Z2016 <- Zmean[,1:8]
Z2016 <- Zmean[,c(1:8)]
Zmean <- function(x){
y <- apply(x, c(1,2), mean)
y[which(y < 0.05)] <- 0
return(y)
}
Z2016 <- Zmean(x = split2016)
Z2016 <- Z2016[,-9:11]
Z2016 <- Z2016[,-c(9:11)]
Z2017 <- Z2017[,-c(9:11)]
sites <- c(1:16)
specs <- logical()
for(i in 1:ncol(Z2016)){
specs[i] <- as.character(paste("Spec", i, sep = ""))
}
long.dat <- function(x){
colnames(x) <- specs
x$Sites <- sites
x %>%
gather(Spec1:Spec8, key = "Species", value = "OccProb")
}
long2016 <- long.dat(x = Z2016)
long.dat <- function(x){
x <- as.data.frame(x)
colnames(x) <- specs
x$Sites <- sites
x %>%
gather(Spec1:Spec8, key = "Species", value = "OccProb")
}
long2016 <- long.dat(x = Z2016)
long2017 <- long.dat(x = Z2017)
occplot1 <- ggplot(data = long2016, aes(x = Species, y = Sites, fill = OccProb))+
geom_tile()+
scale_fill_gradient(low = "white", high = "black")
occplot1
occplot2 <- ggplot(data = long2017, aes(x = Species, y = Sites, fill = OccProb))+
geom_tile()+
scale_fill_gradient(low = "white", high = "black")
occplot2
occplot1
occplot2
occplot1 <- ggplot(data = long2016, aes(x = Species, y = Sites, fill = OccProb))+
geom_tile()+
scale_fill_gradient(low = "white", high = "black")+
theme_bw()
occplot2 <- ggplot(data = long2017, aes(x = Species, y = Sites, fill = OccProb))+
geom_tile()+
scale_fill_gradient(low = "white", high = "black")+
theme_bw()
occplot1
occplot2
occplot1 <- ggplot(data = long2016, aes(x = Species, y = Sites, fill = OccProb))+
geom_tile()+
scale_fill_gradient(low = "white", high = "black")+
theme_bw(base_size = 16)
occplot2 <- ggplot(data = long2017, aes(x = Species, y = Sites, fill = OccProb))+
geom_tile()+
scale_fill_gradient(low = "white", high = "black")+
theme_bw(base_size = 16)
occplot1
occplot2
library(arcgisbinding)
arc.check_product()
dataset <- arc.open(path = "C:\\EsriTraining\\R-ArcGISBridge\\crashes_per_mile.shp")
filtered.df <- arc.select(dataset,
fields=c('RouteNm','C_MI'),
where_clause="RouteNm= 'I-15' AND FC_NAME = 'Urban Interstate'")
crashes.mean <- mean(filtered.df$C_MI)
library(devtools)
install_github("weecology/ratdat")
#Load packages and data -------------------------------------------
library(ratdat)
ratdat <- ratdat
ratdat <- ratdat()
ratdat <- ratdat
ratdat <- ratdat::surveys
ratdat <- surveys
View(ratdat)
compdat <- complete.cases(ratdat)
compdat <- ratdat[complete.cases(ratdat)]
compdat <- ratdat[complete.cases(ratdat),]
View(compdat)
#Load packages and data --------------------------------------------------------
library(glmnet)
swiss <- swiss
View(swiss)
#Create a vector of lambdas
lambdas <- seq(-5, 5, length.out = 10)
swiss.x <- swiss[-swiss$Fertility]
View(swiss.x)
swiss.x <- swiss[,-swiss$Fertility]
swiss.x <- swiss[-swiss$Fertility]
swiss.x <- data.frame(swiss$Agriculture:swiss$Infant.Mortality)
View(swiss.x)
swiss.x <- swiss[,-1]
View(swiss.x)
#Write functions for multiple lasso regressions -------------------------------
run.lasso <- function(x, y, lambdas){
for(i in 1:length(lambdas)){
lasso.mod <- glmnet(x = swiss.x, y = swiss.y, alpha = 0, lambda = lambdas[i])
}
}
#Write functions for multiple lasso regressions -------------------------------
run.lasso <- function(x, y, lambdas){
for(i in 1:length(lambdas)){
lasso.mod <- list()
lasso.mod[[i]] <- glmnet(x = swiss.x, y = swiss.y, alpha = 0,
lambda = lambdas[i])
return(lasso.mod)
}
}
lassos <- run.lasso(x = swiss.x, y = swiss.y, lambdas = lambdas)
swiss.y <- swiss$Fertility
lassos <- run.lasso(x = swiss.x, y = swiss.y, lambdas = lambdas)
#Create a vector of lambdas
lambdas <- seq(0, 5, length.out = 10)
#Write functions for multiple lasso regressions -------------------------------
run.lasso <- function(x, y, lambdas){
for(i in 1:length(lambdas)){
lasso.mod <- list()
lasso.mod[[i]] <- glmnet(x = x, y = y, alpha = 0,
lambda = lambdas[i])
return(lasso.mod)
}
}
#Create a vector of lambdas
lambdas <- seq(0, 5, length.out = 10) #lambdas must be positive
#Write functions for multiple lasso regressions -------------------------------
run.lasso <- function(x, y, lambdas){
for(i in 1:length(lambdas)){
lasso.mod <- list()
lasso.mod[[i]] <- glmnet(x = x, y = y, alpha = 0,
lambda = lambdas[i])
return(lasso.mod)
}
}
lassos <- run.lasso(x = swiss.x, y = swiss.y, lambdas = lambdas)
swiss.x <- model.matrix(Fertility~., swiss)[,-1]
#Write functions for multiple lasso regressions -------------------------------
run.lasso <- function(x, y, lambdas){
for(i in 1:length(lambdas)){
lasso.mod <- list()
lasso.mod[[i]] <- glmnet(x = x, y = y, alpha = 0,
lambda = lambdas[i])
return(lasso.mod)
}
}
lassos <- run.lasso(x = swiss.x, y = swiss.y, lambdas = lambdas)
View(lassos)
#Write functions for multiple lasso regressions -------------------------------
run.lasso <- function(x, y, lambdas){
for(i in 1:length(lambdas)){
lasso.mod <- list()
lasso.mod[[i]] <- glmnet(x = x, y = y, alpha = 0,
lambda = lambdas[i])
}
return(lasso.mod)
}
lassos <- run.lasso(x = swiss.x, y = swiss.y, lambdas = lambdas)
View(lassos)
lassos[[1]]
library(glmnet)
swiss <- swiss
swiss.y <- swiss$Fertility
swiss.x <- model.matrix(Fertility~., swiss)[,-1]
#Create a vector of lambdas
lambdas <- seq(0, 5, length.out = 10) #lambdas must be positive
#Write functions for multiple lasso regressions -------------------------------
run.lasso <- function(x, y, lambdas){
for(i in 1:length(lambdas)){
lasso.mod <- list()
lasso.mod[[i]] <- glmnet(x = x, y = y, alpha = 0,
lambda = lambdas[i])
}
return(lasso.mod)
}
lassos <- run.lasso(x = swiss.x, y = swiss.y, lambdas = lambdas)
lasso.mod[[1]] <- glmnet(x = x, y = y, alpha = 0,
lambda = lambdas[1])
lasso.mod[[1]] <- glmnet(x = swiss.x, y = swiss.y, alpha = 0,
lambda = lambdas[1])
lasso.mod <- list()
lasso.mod[[1]] <- glmnet(x = swiss.x, y = swiss.y, alpha = 0,
lambda = lambdas[1])
#Write functions for multiple lasso regressions -------------------------------
run.lasso <- function(x, y, lambdas){
lass.mod <- list()
for(i in 1:length(lambdas)){
lasso.mod[[i]] <- glmnet(x = x, y = y, alpha = 0,
lambda = lambdas[i])
}
return(lasso.mod)
}
lassos <- run.lasso(x = swiss.x, y = swiss.y, lambdas = lambdas)
lassos[[1]]
summary(lassos[[1]])
summary(lasso.mod)
lasso.mod
lasso.mod <- glmnet(x = swiss.x, y = swiss.y, alpha = 0,
lambda = lambdas[1])
lasso.mod$beta
lasso.mod$call
lasso.mod <- glmnet(x = swiss.x, y = swiss.y, alpha = 0,
lambda = lambdas[10])
lasso.mod$beta
### Step 1: Load and initialize the arcgisbinding
library(arcgisbinding)
arc.check_product()
install.packages("mclust")
library(sp)
library(mclust)
### Step 4: Load the billboard faces dataset
d <- arc.open(path = 'c:\\EsriTraining\\R-ArcGISBridge\\data.gdb\\Billboard_Faces')
library(tidyverse)
### Step 5: Select a subset data from the loaded dataset
dsub <- select(d, WIDTH, HEIGHT, OBJECTID)
d %>%
as.data.frame()
d %>%
arc.select(c('WIDTH','HEIGHT','OBJECTID'),
where_clause = "WIDTH > 25 AND HEIGHT > 11")
### Step 4: Load the billboard faces dataset
d <- arc.open(path = 'c:\\EsriTraining\\R-ArcGISBridge\\data.gdb\\Billboard_Faces')
d %>%
arc.select(c('WIDTH','HEIGHT','OBJECTID'),
where_clause = "WIDTH > 25 AND HEIGHT > 11") %>%
{. ->> data}
d %>%
arc.select(c('WIDTH','HEIGHT','OBJECTID'),
where_clause = "WIDTH > 25 AND HEIGHT > 11")
### Step 4: Load the billboard faces dataset
d <- arc.open(path = 'c:\\EsriTraining\\R-ArcGISBridge\\data.gdb\\Billboard_Faces')
data <- arc.select(c('WIDTH','HEIGHT','OBJECTID'),
where_clause = "WIDTH > 25 AND HEIGHT > 11")
### Step 4: Load the billboard faces dataset
d <- arc.open(path = 'c:\\EsriTraining\\R-ArcGISBridge\\data.gdb\\Billboard_Faces')
data <- arc.select(c('WIDTH','HEIGHT','OBJECTID'),
where_clause = "WIDTH > 25 AND HEIGHT > 11")
data <- arc.select(d, c('WIDTH','HEIGHT','OBJECTID'),
where_clause = "WIDTH > 25 AND HEIGHT > 11")
### Step 6: Get the shape object from the selected subset data
shape <- arc.shape(data)
### Step 7: Pull the x and y coordinate values for each record and populate a dataframe
xy <- data.frame(shape$x, shape$y)
patternBIC <- mclustBIC(data.xy)
### takes  the cluster results, and the data.xy dataframe and produces a summary
patternModel <- summary(patternBIC, data.xy)
### returns a list of probabilities
n <- patternModel$G
cond_probs <- lapply(1:n, function(i) patternModel$z[,i])
### determines the best model from clustering
bestModel <- mclustModel(data.xy, patternBIC)
### Calls the create.ellipse function
polygons <- create.ellipses(bestModel)
### pulls the mean value parameters from the bestmodel
mu <- bestModel$parameters$mean
### Step 8: Create a SpatialPolygonsDataFrame object to hold the polygon
### Step 9: Convert an sp object to ArcGIS object
### Step 10: Write the output of the clustering to a new feature class
### utility functions for creating the ellipses
create.ellipses <- function(bestModel)
{
n <- bestModel$G
mu <- bestModel$parameters$mean
sigma <- bestModel$parameters$variance$sigma
cls.polygons <- lapply(1:n, function(i)
{
xy <- make.ellipse(mu = mu[,i], sigma = sigma[, , i])
name <- paste0("ellipse", i)
Polygons(list(Polygon(xy)), name)
})
SpatialPolygons(cls.polygons, 1:n, proj4string=sp::CRS())
}
make.ellipse <- function(mu, sigma, k = 60)
{
p <- length(mu)
if(p != 2)
stop("only two-dimensional case is available")
if(any(unique(dim(sigma)) != p))
stop("mu and sigma are incompatible")
ev <- eigen(sigma, symmetric = TRUE)
s <- sqrt(rev(sort(ev$values)))
v <- t(ev$vectors[, rev(order(ev$values))])
theta <- (0:k) * (2*pi/k)
x <- s[1] * cos(theta)
y <- s[2] * sin(theta)
xy <- cbind(x,y)
xy <- xy %*% v
cbind(xy[,1] + mu[1], xy[,2] + mu[2])
}
?check_product
?check_product()
??check_product
install.packages("lars")
#Load packages and data ---------------------------------------------------------
library(lars)
library(glmnet)
diabetes <- diabetes
diabetes <- diabetes()
#Load packages and data ---------------------------------------------------------
library(lars)
diabetes <- diabetes()
diabetes <- diabetes
diabetes
data(diabetes)
#Run a lm for benchmark comparisons
bench.model <- lm(data = diabetes, formula = y ~ x)
summary(bench.model)
#Plot coefficient shrinkage using glmnet ----------------------------------------
shrink <- glmnet(x = diabetes$x, y = diabetes$y)
plot.glmnet(x = shrink, xvar = "norm", label = T)
plot.glmnet(x = shrink, xvar = "lambda", label = T)
#Find the value of lambda that minimizes mean cross validation error ------------
cverror <- cv.glmnet(x = diabetes$x, y = diabetes$y, alpha = 1)
plot.cv.glmnet(x = cverror)
cverror$lambda.min
#Use the above lambda value to get the estimated beta (coefficient) matrix ------
shrink.fit <- glmnet(x = diabetes$x, y = diabetes$y, lambda = cverror$lambda.min)
shrink.fit$beta
#Find the value of lambda that minimizes mean cross validation error ------------
cverror <- cv.glmnet(x = diabetes$x, y = diabetes$y, alpha = 1, lambda = 1000)
#Find the value of lambda that minimizes mean cross validation error ------------
cverror <- cv.glmnet(x = diabetes$x, y = diabetes$y, alpha = 1, nlambda = 1000)
plot.cv.glmnet(x = cverror)
cverror$lambda.min
#Use the above lambda value to get the estimated beta (coefficient) matrix ------
shrink.fit <- glmnet(x = diabetes$x, y = diabetes$y, lambda = cverror$lambda.min)
shrink.fit$beta
#Use a larger value of lambda to further reduce the number of predictors --------
cverror$lambda.1se
shrink.fit2 <- glmnet(x - diabetes$x, y = diabetes$y, lambda = cverror$lambda.1se)
shrink.fit2 <- glmnet(x = diabetes$x, y = diabetes$y, lambda = cverror$lambda.1se)
shrink.fit2$beta
#Setup -------------------------------------------------------------------------
library(vcdExtra)
library(vegan)
library(R2OpenBUGS)
library(abind)
library(tidyverse)
setwd("c:/users/beasley/dropbox/MSAMsims")
set.seed(15) #ensures sim is same each time
#Prelim data: sites, survey, seed -----------------------------------------------
J <- 30 #sites
K <- 3 #surveys per site
specs<-11 #Number of species
Ks<-rep(K, J)
#Ks is a vector of length J indicationg # of sampling periods per site
#simulated values for covs would go here
#Simulating abundance data --------------------------------------------------
mean.lambdas <- rlogseries(specs, 0.75)
#Draw lambdas from a logseries distribution
alpha0 <- log(mean.lambdas) #log-scale intercept
#abundance responses to any covariates would go here as alpha1, alpha2, etc
log.lambdas <- alpha0  #this is your log link function. add covs here
lambdas <- exp(log.lambdas)  #inverse link transformation
#most of these steps won't matter until covs are added
#create list of abundance vectors
nlist<-list()
for(a in 1:specs){
nlist[[a]] <- rpois(n = J, lambda = lambdas[a])
}
ns<-do.call(rbind, nlist) #turn abundance vectors into abundance matrix
rowSums(ns) #total abundances
rotate.ns<-t(ns) #I might need an inverted matrix later
#Simulated observation process ------------------------------------------
some.det <- runif(n = specs-1, min = 0, max = 0.6)#simulate mean detection probs
#These are fairly low det probs
no.det <- 0
#Two species will not be detected
mean.det <- c(some.det, no.det)
beta0<-qlogis(mean.det) #put it on logit scale
#responses to detection covs would go here
logit.p<-beta0 #logit link function. Det covs go here
p <- plogis(logit.p) #Transform it back
#Simulate observation data
L<-list()
for(b in 1:specs){
y<-matrix(NA, ncol = K, nrow = J)
for(a in 1:K){
y[,a]<-rbinom(n = J, size = ns[b,], prob = p[b])
}
L[[b]]<-y
}
#I suppose I could make that a function but I'm lazy
#Smash it into array
obsdata<-array(as.numeric(unlist(L)), dim=c(J, K, specs-1))
#Nondetected species were removed from observation data
#Number of observed species
n <- length(some.det)
#Augment data with all-zero matrices
n.aug <- 2
augmats <- array(0, dim = c(J, K, n.aug))
augdata <- abind(obsdata, augmats, along = 3)
maxobs <- apply(augdata, c(1,3), max)
#Write model and send to Gibbs sampler ------------------------------------------
cat("
model{
#Define hyperprior distributions
omega ~ dunif(0,1)
a0.mean ~ dunif(0,1)
mu.a0 <- log(a0.mean)-log(1-a0.mean)
tau.a0 ~ dgamma(0.1, 0.1)
b0.mean ~ dunif(0,1)
mu.b0 <- log(b0.mean)-log(1-b0.mean)
tau.b0 ~ dgamma(0.1, 0.1)
for(i in 1:(n+n.aug)){
#create priors from distributions above
w[i] ~ dbern(omega)
#w[i] indicates whether or not species is exposed to sampling
a0[i] ~ dnorm(mu.a0, tau.a0)
b0[i] ~ dnorm(mu.b0, tau.b0)
#Loop within a loop to estimate abund of spec i at site j
for(j in 1:J){
lambda[j,i] <- exp(a0[i])
mu.lambda[j,i] <- lambda[j,i]*w[i]
Z[j,i] ~ dpois(mu.lambda[j,i])
#Z is the estimated abundance matrix
#Loop within loops for estimating det of spec i at site j at time k
for(k in 1:K[j]){
p[j,k,i] <- b0[i]
logit.p[j,k,i] <- 1 / (1 + exp(-p[j,k,i]))
obsdata[j,k,i] ~ dbin(logit.p[j,k,i], Z[j,i])
}
}
}
#Estimate total richness (N) by adding observed (n) and unobserved (n0) species
n0<-sum(w[(n+1):(n+n.aug)])
N<-n+n0
}
", file = "augmentsanscovs.txt")
#Compile data
datalist<-list(n = n, n.aug = n.aug, J=J, K=Ks, obsdata=augdata)
#Specify parameters to return to R
params<-list('Z','lambda','a0','b0', 'mu.a0', 'mu.b0', 'tau.a0', 'tau.b0', 'N')
#Generate initial values
init.values<-function(){
omega.guess <- runif(1,0,1)
lambda.guess <- runif(1,0,5)
list(omega = omega.guess,
w=c(rep(1,n), rbinom(n = n.aug,size=1,prob=omega.guess)),
a0 = rnorm(n = (n+n.aug), mean = mean(alpha0)),
b0 = rnorm(n = (n+n.aug), mean = runif(1,0,1)),
Z = maxobs
)
}
augmodel <- readRDS(file = "augsanscovs.RDS")
augmodel
augmodel <- bugs(model.file = "augmentsanscovs.txt", data = datalist, n.chains = 3,
parameters.to.save = params, inits = init.values,
n.burnin = 12000, n.iter = 15000, debug = T)
augmodel
augmodel <- readRDS(file = "augsanscovs.RDS")
augmodel
augmodel <- bugs(model.file = "augmentsanscovs.txt", data = datalist, n.chains = 3,
parameters.to.save = params, inits = init.values,
n.burnin = 5000, n.iter = 10000, debug = T)
augmodel
augmodel <- readRDS(file = "augsanscovs.RDS")
#Model evaluation: regional and site-level richness ----------------------------
Ns <- augmodel$sims.list$N
mean(Ns); quantile(Ns, c(0.025, 0.25, 0.75, 0.975))
ggplot()+
geom_histogram(aes(x = Ns), binwidth = 1)
