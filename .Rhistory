arc.select(c('WIDTH','HEIGHT','OBJECTID'),
where_clause = "WIDTH > 25 AND HEIGHT > 11")
### Step 4: Load the billboard faces dataset
d <- arc.open(path = 'c:\\EsriTraining\\R-ArcGISBridge\\data.gdb\\Billboard_Faces')
data <- arc.select(c('WIDTH','HEIGHT','OBJECTID'),
where_clause = "WIDTH > 25 AND HEIGHT > 11")
### Step 4: Load the billboard faces dataset
d <- arc.open(path = 'c:\\EsriTraining\\R-ArcGISBridge\\data.gdb\\Billboard_Faces')
data <- arc.select(c('WIDTH','HEIGHT','OBJECTID'),
where_clause = "WIDTH > 25 AND HEIGHT > 11")
data <- arc.select(d, c('WIDTH','HEIGHT','OBJECTID'),
where_clause = "WIDTH > 25 AND HEIGHT > 11")
### Step 6: Get the shape object from the selected subset data
shape <- arc.shape(data)
### Step 7: Pull the x and y coordinate values for each record and populate a dataframe
xy <- data.frame(shape$x, shape$y)
patternBIC <- mclustBIC(data.xy)
### takes  the cluster results, and the data.xy dataframe and produces a summary
patternModel <- summary(patternBIC, data.xy)
### returns a list of probabilities
n <- patternModel$G
cond_probs <- lapply(1:n, function(i) patternModel$z[,i])
### determines the best model from clustering
bestModel <- mclustModel(data.xy, patternBIC)
### Calls the create.ellipse function
polygons <- create.ellipses(bestModel)
### pulls the mean value parameters from the bestmodel
mu <- bestModel$parameters$mean
### Step 8: Create a SpatialPolygonsDataFrame object to hold the polygon
### Step 9: Convert an sp object to ArcGIS object
### Step 10: Write the output of the clustering to a new feature class
### utility functions for creating the ellipses
create.ellipses <- function(bestModel)
{
n <- bestModel$G
mu <- bestModel$parameters$mean
sigma <- bestModel$parameters$variance$sigma
cls.polygons <- lapply(1:n, function(i)
{
xy <- make.ellipse(mu = mu[,i], sigma = sigma[, , i])
name <- paste0("ellipse", i)
Polygons(list(Polygon(xy)), name)
})
SpatialPolygons(cls.polygons, 1:n, proj4string=sp::CRS())
}
make.ellipse <- function(mu, sigma, k = 60)
{
p <- length(mu)
if(p != 2)
stop("only two-dimensional case is available")
if(any(unique(dim(sigma)) != p))
stop("mu and sigma are incompatible")
ev <- eigen(sigma, symmetric = TRUE)
s <- sqrt(rev(sort(ev$values)))
v <- t(ev$vectors[, rev(order(ev$values))])
theta <- (0:k) * (2*pi/k)
x <- s[1] * cos(theta)
y <- s[2] * sin(theta)
xy <- cbind(x,y)
xy <- xy %*% v
cbind(xy[,1] + mu[1], xy[,2] + mu[2])
}
?check_product
?check_product()
??check_product
install.packages("lars")
#Load packages and data ---------------------------------------------------------
library(lars)
library(glmnet)
diabetes <- diabetes
diabetes <- diabetes()
#Load packages and data ---------------------------------------------------------
library(lars)
diabetes <- diabetes()
diabetes <- diabetes
diabetes
data(diabetes)
#Run a lm for benchmark comparisons
bench.model <- lm(data = diabetes, formula = y ~ x)
summary(bench.model)
#Plot coefficient shrinkage using glmnet ----------------------------------------
shrink <- glmnet(x = diabetes$x, y = diabetes$y)
plot.glmnet(x = shrink, xvar = "norm", label = T)
plot.glmnet(x = shrink, xvar = "lambda", label = T)
#Find the value of lambda that minimizes mean cross validation error ------------
cverror <- cv.glmnet(x = diabetes$x, y = diabetes$y, alpha = 1)
plot.cv.glmnet(x = cverror)
cverror$lambda.min
#Use the above lambda value to get the estimated beta (coefficient) matrix ------
shrink.fit <- glmnet(x = diabetes$x, y = diabetes$y, lambda = cverror$lambda.min)
shrink.fit$beta
#Find the value of lambda that minimizes mean cross validation error ------------
cverror <- cv.glmnet(x = diabetes$x, y = diabetes$y, alpha = 1, lambda = 1000)
#Find the value of lambda that minimizes mean cross validation error ------------
cverror <- cv.glmnet(x = diabetes$x, y = diabetes$y, alpha = 1, nlambda = 1000)
plot.cv.glmnet(x = cverror)
cverror$lambda.min
#Use the above lambda value to get the estimated beta (coefficient) matrix ------
shrink.fit <- glmnet(x = diabetes$x, y = diabetes$y, lambda = cverror$lambda.min)
shrink.fit$beta
#Use a larger value of lambda to further reduce the number of predictors --------
cverror$lambda.1se
shrink.fit2 <- glmnet(x - diabetes$x, y = diabetes$y, lambda = cverror$lambda.1se)
shrink.fit2 <- glmnet(x = diabetes$x, y = diabetes$y, lambda = cverror$lambda.1se)
shrink.fit2$beta
install.packages("FactoMineR")
install.packages("factoextra")
library(FactoMineR)
library(FactoMineR)
library(factoextra)
data("iris")
head(iris)
iris2 <- iris[,1:4]
#Run the PCA
iris.pca <- PCA(iris2, scale.unit = T, graph = F)
#Look at eigenvalues
iris.pca$eig
#scree plot for visualization
fviz_screeplot(iris.pca, ncp = 4)
plot.PCA(iris.pca, axes = c(1,2), choix = "var")
#biplot
fviz_pca(iris.pca)
#Clean it up
fviz_pca_var(iris.pca, col.var = "contrib")
#Clean it up with a scale showing relative contributions
fviz_pca_var(iris.pca, col.var = "contrib")+
scale_color_gradient2(low = "blue", mid = "steelblue", high = "red",
midpoint = 25)+
theme_bw()
#individual data points without lables
fviz_pca_ind(iris.pca, label = "none")
#add some color to make clusters more informative
fviz_pca_ind(iris.pca, label = "none", habillage = iris$Species)
#add some color to make clusters more informative
fviz_pca_ind(iris.pca, label = "none", habillage = iris$Species, ellipseCA(0.95))
#add some color to make clusters more informative
fviz_pca_ind(iris.pca, label = "none", habillage = iris$Species, ellipse.level = 0.95
)
#add some color to make clusters more informative
fviz_pca_ind(iris.pca, label = "none", habillage = iris$Species, addEllipses = T
ellipse.level = 0.95)
#add some color to make clusters more informative
fviz_pca_ind(iris.pca, label = "none", habillage = iris$Species, addEllipses = T,
ellipse.level = 0.95)
fviz_pca_biplot(iris.pca, geom.ind = "point", fill.ind = iris$Species,
col.ind = "black", addEllipses = T, col.var = "contrib")
#Creating web-based graphs with plotly----------------------------------
library(plotly)
install.packages("plotly")
#Creating web-based graphs with plotly----------------------------------
#load packas
library(plotly)
data(mpg)
data <- mpg
p <- plot_ly(data = mpg, x = ~displ, y = ~cty)
p
#plotly also interacts with ggplot with ggplotly
p2 <- ggplotly(data = mpg, aes(x = displ, y = cty))
#plotly also interacts with ggplot with ggplotly
p2 <- ggplot(data = mpg, aes(x = displ, y = cty))+
geom_point()
ggplotly(p2)
#plotly also interacts with ggplot with ggplotly
p2 <- ggplot(data = mpg, aes(x = displ, y = cty, color = manufacturer))+
geom_point()
ggplotly(p2)
head(txhousing)
tx <- txhousing
allCities <- txhousing %>%
group_by(city) %>%
plot_ly(x = ~date, y = ~median)
allcities
allCities
#3d plots
plot_ly(data = iris, x = ~sepal.length, y = ~sepal.width, z = ~petal.length,
type = "scatter3d", mode = "markers", size = petal.width, color = ~Species)
#3d plots
plot_ly(data = iris, x = ~Sepal.Length, y = ~Sepal.Width, z = ~Petal.Length,
type = "scatter3d", mode = "markers", size = Petal.Width, color = ~Species)
#3d plots
plot_ly(data = iris, x = ~Sepal.Length, y = ~Sepal.Width, z = ~Petal.Length,
type = "scatter3d", mode = "markers", size = ~Petal.Width, color = ~Species)
#Animations
df <- data.frame(x = c(1:5, 4:1), y = c(1:5, 4:1), f = (a:9))
#Animations
df <- data.frame(x = c(1:5, 4:1), y = c(1:5, 4:1), f = (1:9))
p <- plot_ly(data = df, x = ~x, y = ~y, frame = ~f, type = "scatter",
transition = ~f, mode = "markers", showleged = T)
p
install.packages("Plot3D")
install.packages("plot3D")
install.packages("plot3Drgl")
install.packages("diffdf")
install.packages("ggtree")
install.packages"treeio"
install.packages("treeio")
install.packages(emojifont)
install.packages("emojifont")
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("ggtree", version = "3.8")
library(betapart)
# Setup -------------------------------------------------------------------------
library(vcdExtra)
library(msm)
library(tidyverse)
set.seed(15)
# Global variables --------------------------------------------------------------
#Mammal
mamm.specs <- 10
sites <- 30
trap.nights <- 3
mamm.genera <- c("genus1", "genus2")
#Ecto
ecto.specs <- 15
life.history <- c("generalist", "specialist", "neither")
# Simulate "true" mammal abundances ---------------------------------------------
mean.lambdas <- rlogseries(mamm.specs, 0.75)
#Draw lambdas from a logseries distribution
alpha0 <- log(mean.lambdas) #log-scale intercept
#abundance responses to any covariates would go here as alpha1, alpha2, etc
log.lambdas <- alpha0  #this is your log link function. add covs here
lambdas <- exp(log.lambdas)  #inverse link transformation
#most of these steps won't matter until covs are added
#create list of abundance vectors
nlist<-list()
for(a in 1:mamm.specs){
nlist[[a]] <- rpois(n = sites, lambda = lambdas[a])
}
#turn abundance vectors into abundance matrix
ns<-do.call(rbind, nlist)
#convert into long format
long.ns <- gather(as.data.frame(ns[,c(1:30)]), key = "Site", value = "Abundance")
long.ns$Species <- rep(1:mamm.specs, sites)
long.ns %>%
arrange(Species) %>%
{. ->> long.ns}
# Simulate mammal observation process (detection error) ---------------------------
#simulate mean detection probs
mean.det <- runif(n = mamm.specs, min = 0.4, max = 0.8)
#These are mid to high detection probabilities
beta0<-qlogis(mean.det) #put it on logit scale
#responses to detection covs would go here
logit.p<-beta0 #logit link function. Det covs go here
p <- plogis(logit.p) #Transform it back
#Simulate observation data
L<-list()
for(b in 1:mamm.specs){
y<-matrix(NA, ncol = trap.nights, nrow = sites)
for(a in 1:trap.nights){
y[,a]<-rbinom(n = sites, size = ns[b,], prob = p[b])
}
L[[b]]<-y
}
#Smash it into array
obsdata<-array(as.numeric(unlist(L)), dim=c(sites, trap.nights, mamm.specs))
long.obs <- matrix(ncol = 3)
for(i in 1:dim(obsdata)[3]){
long.obs <- rbind(long.obs, obsdata[,,i])
}
long.obs <- long.obs[-1,]
# Occupancy probs for each host/ecto species pair ---------------------------------
#Generate name vectors for mammals and ectos
mamm.names <- logical()
for(i in 1:mamm.specs){
mamm.names[i] <- as.character(paste("Mamm", i, sep = ""))
}
ecto.names <- logical()
for(i in 1:ecto.specs){
ecto.names[i] <- as.character(paste("Ecto", i, sep = ""))
}
#Generate mammal and ecto data frames
mammdat <- data.frame(mammspec = mamm.names,
mammgenus = sample(mamm.genera, size = mamm.specs,
replace = T))
ectodat <- data.frame(ecto.names = ecto.names,
life.history = sample(life.history, size = ecto.specs,
replace = T))
#Generate occupancy probabilities. Commence for loop of hell.
genus <- logical()
occprobs <- matrix(NA, nrow = ecto.specs, ncol = mamm.specs)
for(i in 1:ecto.specs){
#Assign a mammal genus to the ecto species
genus <- sample(mammdat$mammgenus, size = 1)
#Generalist species: can occupy all species in a genus
if(ectodat$life.history[i] == "generalist"){
#Generate occupancy probs for all mamm in the genus
for(j in 1:mamm.specs){
if(mammdat$mammgenus[j] == genus){
occprobs[i,j] <- runif(1, 0, 1)
} else{
occprobs[i,j] <- 0
}
}
#Specialist species: Only one host
} else if(ectodat$life.history[i] == "specialist"){
#Generate an occupancy prob for a single host
host <- sample(mammdat$mammspec, size = 1)
for(j in 1:length(mammdat$mammspec))
if(mammdat$mammspec[j] == host){
occprobs[i,j] <- runif(1,0,1)
} else{
occprobs[i,j] <- 0
}
#"Neither" category: one primary and one secondary host
} else if(ectodat$life.history[i] == "neither"){
#Generate occupancy prob for a primary and secondary host
somehosts <- sample(which(mammdat$mammgenus == genus), size = 2)
occprobs[i, somehosts[1]] <- runif(1, 0.5, 1) #primary host
occprobs[i, somehosts[2]] <- runif(1, 0, 0.5) #secondary host
occprobs[is.na(occprobs)] <- 0
}
}
# Get occupancy probabilities for each host population based on above matrix -----
popprobs <- function(x, y, z, mat){
pop.array <- array(dim = c(x, y, z))
for(i in 1:x){
for(j in 1:y){
for(k in 1:z){
if(mat[k,i] != 0){
pop.array[i,j,k] <- rtnorm(n = 1, mean = mat[k,i], sd = 0.05,
lower = 0, upper = 1)
#Use a truncated normal distribution to make sure it's a probability
#May use a different distribution later
} else{
pop.array[i,j,k] <- 0
}
}
}
}
#Convert array to a matrix
popoccs <- matrix(NA, nrow = x*y, ncol = z)
for(i in 1:z){
popoccs[,i] <- as.vector(pop.array[,,i])
}
popoccs <- as.data.frame(popoccs)
}
pop.probs <- popprobs(x = mamm.specs, y = sites, z = ecto.specs, mat = occprobs)
#Make sure ecto occupancy prob is 0 if there are no hosts present
for(i in 1:(mamm.specs*sites)){
if(long.ns$Abundance[i] == 0){
pop.probs[i,] <- rep(0, ecto.specs)
}
}
# Use occupancy probs to get "true" presence/absence data
occstate <- matrix(NA, nrow = mamm.specs*sites, ncol = ecto.specs)
for(i in 1:(mamm.specs*sites)){
for(j in 1:ecto.specs){
occstate[i,j] <- as.numeric(rbernoulli(n = 1, p = pop.probs[i,j]))
}
}
#Create "true" ecto occupancy for individual rodents -------------------------
#Simulate mean occupancy prob for each ectoparasite
#Same individual host prob regardless of host species
ind.occ <- runif(n = ecto.specs, min = 0.3, max = 0.6)
#These are quite high but I want at least some non-zero data to wor with
#Parasites also tend to aggregate, but for now I'm not messing with that
#Create detection histories
L <- list()
for(i in 1:ecto.specs){
y <- matrix(NA, nrow = (mamm.specs*sites), ncol = max(long.ns$Abundance))
for(k in 1:(mamm.specs*sites)){
for(j in 1:long.ns$Abundance[k]){
y[k,j] <- as.numeric(rbernoulli(n = 1, p = ind.occ[i]*occstate[k,i]))
}
}
L[[i]] <- y
}
#Change list of matrices into array
ind.obs<-array(unlist(L), dim=c(mamm.specs*sites, max(long.ns$Abundance),
ecto.specs))
# Create probability of detecting an ecto at each capture -----------------------
#Create individual mammal IDs
IDs <- list()
for(i in 1:length(long.ns$Abundance)){
if(long.ns$Abundance[i] != 0){
IDs[[i]] <- seq(1:long.ns$Abundance[i])
} else{
IDs[[i]] <- NA
}
}
#Find out which individuals of the population were sampled during period k
samples <- list()
for(i in 1:length(IDs)){
samples[[i]] <- list()
for(j in 1:dim(long.obs)[2]){
if(is.na(IDs[[i]]) == F){
samples[[i]][[j]] <- sample(IDs[[i]], size = long.obs[i,j])
} else{
samples[[i]] <- NA
}
}
}
install.packages("popbio")
#Load libraries
library(tidyverse)
library(patchwork)
library(betapart)
install.packages("caret")
install.packages("rpart")
library(devtools)
install_github("dphansti/Sushi")
library(vcdExtra)
library(TeachingDemos)
library(ggplot2)
library(R2OpenBUGS)
setwd("c:/users/beasley/dropbox/MSAMsims")
#Prelim data: sites, survey, seed -----------------------------------------------
set.seed(15)
J <- 30 #sites
K <- 3 #surveys per site
specs<-11 #Number of species
Ks<-rep(K, J)
#Ks is a vector of length J indicationg # of sampling periods per site
#Site covariate (abundance)
cov1<-rnorm(n = J, mean = 10, sd = 3)
cov1scale <- as.vector(scale(cov1))
#Survey covariate (detection)
detcov <- matrix(runif(J*K, 0, 10), nrow = J, ncol = K)
#Simulating abundance data --------------------------------------------------
mean.lambdas <- rlogseries(specs, 0.75) #Draw lambdas from a logseries distribution
mean.lambdas[11] <- 0.5 #nondetected species should have low abundance for realism
#log-scale intercept
alpha0 <- log(mean.lambdas)
#response to site covariate: all species have positive response
alpha1 <- alpha1 <- rep(1, specs)
#Get new lambdas
log.lambdas <- matrix(NA, nrow = specs, ncol = J)
for(i in 1:specs){
log.lambdas[i,] <- alpha0[i] + alpha1[i]*cov1scale #log link function
}
#inverse link transformation
lambdas <- exp(log.lambdas)
#create list of abundance vectors
ns <- matrix(NA, nrow = specs, ncol = J)
for(a in 1:specs){
ns[a,] <- rpois(n = J, lambda = lambdas[a,])
}
rowSums(ns) #total abundances
View(ns)
mean.lambdas[11] <- 0.25 #nondetected species should have low abundance for realism
#log-scale intercept
alpha0 <- log(mean.lambdas)
#response to site covariate: all species have positive response
alpha1 <- alpha1 <- rep(1, specs)
#Get new lambdas
log.lambdas <- matrix(NA, nrow = specs, ncol = J)
for(i in 1:specs){
log.lambdas[i,] <- alpha0[i] + alpha1[i]*cov1scale #log link function
}
#inverse link transformation
lambdas <- exp(log.lambdas)
#create list of abundance vectors
ns <- matrix(NA, nrow = specs, ncol = J)
for(a in 1:specs){
ns[a,] <- rpois(n = J, lambda = lambdas[a,])
}
rowSums(ns) #total abundances
#Detection intercept and cov responses
beta0<-qlogis(mean.det) #put it on logit scale
#Simulated observation process --------------------------------------------
mean.det <- runif(n = specs, min = 0.2, max = 0.6) #simulate mean detection probs
#These are low to mid detection values
mean.det[11] <- 0 #one species was not detected
#Detection intercept and cov responses
beta0<-qlogis(mean.det) #put it on logit scale
#Detection cov does not affect detectability
beta1 <- rnorm(n = specs, mean = 0, sd = 0.01)
#Logit link function
logit.p <- array(NA, dim = c(J, K, specs))
for(i in 1:specs){
logit.p[,,i] <- beta0[i] + beta1[i]*detcov
}
p <- plogis(logit.p)
#Simulate observation data
L<-list()
for(b in 1:specs){
y<-matrix(NA, ncol = K, nrow = J)
for(a in 1:K){
y[,a]<-rbinom(n = J, size = ns[b,], prob = p[,,b])
}
L[[b]]<-y
}
#Smash it into array
obsdata<-array(as.numeric(unlist(L)), dim=c(J, K, specs))
#Convert obsdata into abundance matrix
maxobs<-apply(obsdata, c(1,3), max)
View(maxobs)
mean.det
#Smash it into array
obsdata<-array(as.numeric(unlist(L)), dim=c(J, K, specs-1))
#Convert obsdata into abundance matrix
maxobs<-apply(obsdata, c(1,3), max)
#Simulated observation process --------------------------------------------
some.det <- runif(n = specs - 1, min = 0.2, max = 0.6) #simulate detection probs
#These are low to mid detection values
no.det <- 0 #one species was not detected
mean.det <- c(some.det, no.det)
#Sanity check: convert obs data into abundance matrix
maxobs<-apply(obsdata, c(1,3), max)
print(maxobs)
#Number of observed species
n <- length(some.det)
#Augment data with all-zero matrices
n.aug <- 2
augmats <- array(0, dim = c(J, K, n.aug))
augdata <- abind(obsdata, augmats, along = 3)
library(abind)
maxobs <- apply(augdata, c(1,3), max)
augdata <- abind(obsdata, augmats, along = 3)
maxobs <- apply(augdata, c(1,3), max)
View(maxobs)
